{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52675a9e-b72c-4215-967a-a5406225aea4",
   "metadata": {},
   "source": [
    "## L2 Language Learner Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c809d722-dcd3-41fd-a4cb-4db4b88eccd5",
   "metadata": {},
   "source": [
    "Building a classifier to distinguish English text written by Lang-8 users whose native language (L1) is another European language (French and Spanish) from those written by L1 speakers of East Asian languages (Japanese, Korean, and Mandarin Chinese)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df05892-619d-408e-abb1-5cd6094d7a9a",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d76234b1-4cb7-416b-8a67-30b83d1794de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/snehajhaveri/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /Users/snehajhaveri/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download(\"cmudict\")\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a871b9-6068-4704-8300-d70f40257cd5",
   "metadata": {},
   "source": [
    "### Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86b12526-70de-4821-a4d7-c90695a73586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_from_zip(path=\"data/lang-8.zip\"):\n",
    "    \"\"\"\n",
    "    A generator function which reads html documents\n",
    "    as raw text from the zip file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : string\n",
    "        path to the zip file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary of filename and raw text extracted\n",
    "    from the file\n",
    "    \"\"\"\n",
    "    archive = ZipFile(path, \"r\")\n",
    "\n",
    "    for file in archive.namelist()[1:]:\n",
    "        yield {\n",
    "            \"filename\": file,#.removeprefix(\"lang-8/\"),\n",
    "            \"data\": archive.read(file)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd7d5e33-770f-47fe-87a5-b6eb8af54c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_file(path=\"data/lang-8.zip\"):\n",
    "    \"\"\"\n",
    "    A generator function which reads html files from zip\n",
    "    and extracts text and native language from the raw\n",
    "    text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : string\n",
    "        path to the zip file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary of extracted content and native language\n",
    "    of the author\n",
    "    \"\"\"\n",
    "    for data_dict in read_file_from_zip(path):\n",
    "        soup = BeautifulSoup(data_dict[\"data\"])\n",
    "\n",
    "#         author = soup.find_all(\"p\", attrs={\"class\": \"spaced\"})[1].get_text().strip()\n",
    "        native_lang = soup.find(\"li\", attrs={\"data-title\": \"Native language\"}).get_text().strip()\n",
    "        filename = data_dict[\"filename\"]\n",
    "        text = soup.find(\"div\", attrs={\"id\": \"body_show_ori\"}).get_text().strip()\n",
    "\n",
    "        preprocessed_data = {\n",
    "            \"text\": text,\n",
    "#             \"author\": author,\n",
    "            \"native_lang\": native_lang,\n",
    "            \"filename\": filename\n",
    "        }\n",
    "\n",
    "        yield preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d397f65-6fd0-4ddc-9900-f7367a3cbac2",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed28333-976b-4add-803d-0fdaeed686f7",
   "metadata": {},
   "source": [
    "#### Text Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b14e13d-15b8-4f44-8534-f8f38b613145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_length(text):\n",
    "    \"\"\"\n",
    "    Returns the number of words in a text without punctuations. \n",
    "    Counts clitics as separate words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A text from which we find the number of words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    An int which represents the number of words in the text\n",
    "    \"\"\"\n",
    "    non_punc = []\n",
    "    for word in word_tokenize(text):\n",
    "        if word not in string.punctuation:\n",
    "            non_punc.append(word)\n",
    "    return len(non_punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e865286-7ada-4fd7-8a12-5ef9e5b8ecdb",
   "metadata": {},
   "source": [
    "#### Lexical Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a22f923-249b-4274-8656-2dd0dd606b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexical_density(text):\n",
    "    \"\"\"\n",
    "    Returns the lexical density of a text. That is the ratio of open class words.\n",
    "    in the text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A text from which we find the lexical density\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A float which represents the lexical density\n",
    "    \"\"\"\n",
    "    open_class_prefix = {\"N\", \"V\", \"J\", \"R\"}\n",
    "    open_class_total = 0\n",
    "    word_count = 0\n",
    "    if len(text) == 0:\n",
    "        return float(0)\n",
    "    for word, pos in pos_tag(word_tokenize(text)):\n",
    "        if word not in string.punctuation:\n",
    "            word_count += 1\n",
    "            if pos[0] in open_class_prefix:\n",
    "                open_class_total += 1\n",
    "    return open_class_total/word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d5e4bd-21bd-4ae2-947a-6859dd328761",
   "metadata": {},
   "source": [
    "#### Average Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bc3d0d3-12be-4d5e-a576-0030aa1a23d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_sentence_length(text):\n",
    "    \"\"\"\n",
    "    Returns the average sentence length of a text. Does not count punctuations and counts clitics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A text from which we find the average sentence length\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A float which represents the average sentence length\n",
    "    \"\"\"\n",
    "    if len(text) == 0:\n",
    "        return float(0)\n",
    "    sent_lengths = 0\n",
    "    for sentence in sent_tokenize(text):\n",
    "        word_count = 0\n",
    "        for word in word_tokenize(sentence):\n",
    "            if word not in string.punctuation:\n",
    "                word_count += 1\n",
    "        sent_lengths += word_count\n",
    "    return sent_lengths/len(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d884d82-1b29-4efc-a2c9-c1ee617700ac",
   "metadata": {},
   "source": [
    "#### Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c51658f4-3e59-4d4b-8b54-8b4b750d04fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word_length(text):\n",
    "    \"\"\"\n",
    "    Returns the average sentence length of a text. Does not count punctuations \n",
    "    and counts clitics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A text from which we find the average sentence length\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A float which represents the average sentence length\n",
    "    \"\"\"\n",
    "    if len(text) == 0:\n",
    "        return float(0)\n",
    "    word_count = 0\n",
    "    lengths_sum = 0\n",
    "    for word in word_tokenize(text):\n",
    "        if word not in string.punctuation:\n",
    "            lengths_sum += len(word)\n",
    "            word_count += 1\n",
    "    return lengths_sum/word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c585d982-cd1f-45a8-905d-80a03cadeece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_text_length tests pass\n",
      "get_lexical_density tests pass\n",
      "get_average_sentence_length tests pass\n",
      "get_average_word_length tests pass\n"
     ]
    }
   ],
   "source": [
    "s0 = \"\"\n",
    "s1 = \"\"\"I went to the park today. \n",
    "I love going there because I always have so much fun. \n",
    "I invited some friends but they didn't come. \n",
    "That's fine because I met a new person there. \n",
    "He had a dog.\n",
    "\"\"\" #40, \n",
    "s2 = \"I have so much work to do today. I am stressed\" #11\n",
    "\n",
    "# get_text_length\n",
    "assert type(get_text_length(s0)) == int, \"Must be an interger\"\n",
    "assert get_text_length(s0) == 0, \"Empty string must return 0\"\n",
    "assert get_text_length(s1) == 40, \"s1 has 40 words\"\n",
    "assert get_text_length(s2) == 11, \"s2 has 11 words\"\n",
    "print(\"get_text_length tests pass\")\n",
    "\n",
    "assert type(get_lexical_density(s0))== float, \"Must be a float\"\n",
    "assert get_lexical_density(s0) == 0, \"Empty string must return 0\"\n",
    "assert get_lexical_density(s1) == 24/40, \"24 open class words out of 40\"\n",
    "assert get_lexical_density(s2) == 8/11, \"8 open class words out of 40\"\n",
    "print(\"get_lexical_density tests pass\")\n",
    "\n",
    "assert type(get_average_sentence_length(s0)) == float, \"Must be a float\"\n",
    "assert get_average_sentence_length(s0) == 0, \"Empty string must return 0\"\n",
    "assert get_average_sentence_length(s1) == 40/5, \"40 words over the span of 5 sentences\"\n",
    "assert get_average_sentence_length(s2) == 11/2, \"11 words over the span 2 sentences\"\n",
    "print(\"get_average_sentence_length tests pass\")\n",
    "\n",
    "assert type(get_average_word_length(s0)) == float, \"Must be a float\"\n",
    "assert get_average_word_length(s0) == 0, \"Empty string must return 0\"\n",
    "assert get_average_word_length(s1) == 142/40, \"142 total characters spread across 40 words\"\n",
    "assert get_average_word_length(s2) == 35/11, \"35 character spread across 11 words\"\n",
    "print(\"get_average_word_length tests pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c960467-2b5a-4182-b76e-2495f7366686",
   "metadata": {},
   "source": [
    "#### Part of Speech (POS) Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4cee2ba5-73ef-484a-92fe-d0798afbdc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_count(text):\n",
    "    \"\"\"\n",
    "    Counts the number of nouns, verbs and adjectives in a text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A text for which we find the number of nouns, verbs\n",
    "        and adjectives\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of (noun_count: int, verb_count: int, adj_count: int)\n",
    "    which represents the number of nouns, verbs adjectives in the text\n",
    "    respectively\n",
    "    \"\"\"\n",
    "    noun_count = 0\n",
    "    verb_count = 0\n",
    "    adj_count = 0\n",
    "\n",
    "    if len(text) == 0:\n",
    "        return 0, 0, 0\n",
    "\n",
    "    for word, pos in pos_tag(word_tokenize(text)):\n",
    "        if(pos[0] == 'N'):\n",
    "            noun_count += 1\n",
    "        if(pos[0] == 'V'):\n",
    "            verb_count += 1\n",
    "        if(pos == 'JJ'):\n",
    "            adj_count += 1\n",
    "    return noun_count, verb_count, adj_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c7d2ae12-f039-4622-af96-347d637cf359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_pos_count tests pass\n"
     ]
    }
   ],
   "source": [
    "s1 = \"\"\"I went to the park today. \n",
    "I love going there because I always have so much fun. \n",
    "I invited some friends but they didn't come. \n",
    "That's fine because I met a new person there. \n",
    "He had a dog.\"\"\"\n",
    "\n",
    "s2 = \"\"\"Chelsea English School is offering a Summer School Program in Iwaki, Fukushima, a holiday learning experience combining enjoyment of the area's natural beauty and practical lifestyle immersion in the agricultural traditions of this part of Japan.\n",
    "We will be hosted by \"Namakiba\" farm, an agricultural concern run by an Iwaki City cooperative, and activities include handson experience of organic farming,barbecues, local nature sightseeing including swimming in the river and the sea,the local fish market, guesthouses with onsens (hot spas) . The program promises new and fresh experiences in both nature and culture, and time will also be made available for gift shopping. Non-Japanese speakers are also warmly invited, as simultaneous translation into English will be available throughout the e trip.\"\"\"\n",
    "\n",
    "assert get_pos_count(s1) == (6, 10, 3)\n",
    "assert get_pos_count(s2) == (47, 17, 16)\n",
    "\n",
    "print(\"get_pos_count tests pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcce884e-f656-438e-bdf8-c9b8fd9fd66d",
   "metadata": {},
   "source": [
    "#### Out of Vocabulary Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "def826d3-06f3-49ef-9122-95043d35db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_ovv_words(text):\n",
    "    \"\"\"\n",
    "    Gets the number of out-of-vocabulary words in a text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A text for which we find the number of out-of-vocabulary\n",
    "        words is to be found\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The number of oov words in the text\n",
    "    \"\"\"\n",
    "    text_vocab = set(w.lower() for w in text.split() if w.isalpha())\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    ovv_words = text_vocab - english_vocab\n",
    "\n",
    "    return len(ovv_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce181ec3-f831-434b-b855-1e1fca378a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_num_ovv_words tests pass\n"
     ]
    }
   ],
   "source": [
    "s0 = \"\"\n",
    "s1 = \"\"\" I haddd to leaasve earliae since yesterday was so tired.\n",
    "And then I met you.\n",
    "\"\"\" \n",
    "s2 = \"I have so much work to do today. I am stressseed\"\n",
    "assert type(get_num_ovv_words(s0)) == int, \"Must be an interger\"\n",
    "assert get_num_ovv_words(s0) == 0, \"Empty string must return 0\"\n",
    "assert get_num_ovv_words(s1) == 3, \"s1 has 3 words out of vaocab\"\n",
    "assert get_num_ovv_words(s2) == 1, \"s2 has 1 words out of vocab\"\n",
    "print(\"get_num_ovv_words tests pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc565f4-279e-4c7a-8a32-b68dd58afa38",
   "metadata": {},
   "source": [
    "#### Reading Ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6854544-8e1c-4019-899b-fb23421f32b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Code adapted from lab\n",
    "\n",
    "vowels = {\"a\",\"e\",\"i\",\"o\",\"u\",\"y\"}\n",
    "p_dict = cmudict.dict()\n",
    "\n",
    "def get_reading_ease(text):\n",
    "    \"\"\"Returns the reading ease for a text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "       A text for which we find the reading ease.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    reading_ease : float\n",
    "        The reading ease for the text\n",
    "    \"\"\"\n",
    "    syllable_count = 0\n",
    "    word_count = 0\n",
    "\n",
    "    for word in word_tokenize(text):\n",
    "        if word not in string.punctuation:\n",
    "            word_count += 1\n",
    "            if word in p_dict:\n",
    "                for pron in p_dict[word][0]:\n",
    "                    if pron[-1] in ['0','1','2']:\n",
    "                        syllable_count +=1\n",
    "            else:\n",
    "                for j in range(0,len(word)):\n",
    "                    if word[j].lower() in vowels:\n",
    "                         syllable_count= syllable_count+1\n",
    "\n",
    "    reading_ease = (206.835 - (1.015*(word_count/len(sent_tokenize(text))))- (84.6*(syllable_count/word_count)))\n",
    "    return reading_ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee8e7141-0586-4c7f-9ffb-b56310d818bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_reading_ease tests pass\n"
     ]
    }
   ],
   "source": [
    "assert 100 < get_reading_ease(\"I am done, man\") < 140\n",
    "assert -60 < get_reading_ease(\"Felicitations for achieving a thoroughly excellent resolution to an altogether indombidable conundrum of humongous proportions.\") <-20\n",
    "print(\"get_reading_ease tests pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c2989-7726-49d7-b66a-6fd26883aa79",
   "metadata": {},
   "source": [
    "#### Punctuation Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "976bf111-d6fa-461b-bcae-e268bad939d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_punctuations_count(text):\n",
    "    \"\"\"\n",
    "    Returns the number of punctuations in a text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A text for which we find the number of punctuations present\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    punct_count: int\n",
    "                 An integer which represents the number of punctuations in the text\n",
    "    \"\"\"\n",
    "    punct_count = 0\n",
    "    if len(text) == 0:\n",
    "        return 0\n",
    "    for word in word_tokenize(text):\n",
    "        if word in string.punctuation:\n",
    "            punct_count += 1\n",
    "    return punct_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "86852d5c-9062-447f-aa3f-19e0d5af07ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_punctuations_count tests pass\n"
     ]
    }
   ],
   "source": [
    "s1 = \"\"\"I went to the park today. \n",
    "I love going there because I always have so much fun. \n",
    "I invited some friends but they didn't come. \n",
    "That's fine because I met a new person there. \n",
    "He had a dog.\"\"\"\n",
    "\n",
    "s2 = \"\"\"Chelsea English School is offering a Summer School Program in Iwaki, Fukushima, a holiday learning experience combining enjoyment of the area's natural beauty and practical lifestyle immersion in the agricultural traditions of this part of Japan.\n",
    "We will be hosted by \"Namakiba\" farm, an agricultural concern run by an Iwaki City cooperative, and activities include handson experience of organic farming,barbecues, local nature sightseeing including swimming in the river and the sea,the local fish market, guesthouses with onsens (hot spas) . The program promises new and fresh experiences in both nature and culture, and time will also be made available for gift shopping. Non-Japanese speakers are also warmly invited, as simultaneous translation into English will be available throughout the e trip.\"\"\"\n",
    "\n",
    "\n",
    "assert get_punctuations_count(s1) == 5\n",
    "assert get_punctuations_count(s2) == 16\n",
    "\n",
    "print(\"get_punctuations_count tests pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226da160-97ce-4a9e-aca3-b37fefd65ecd",
   "metadata": {},
   "source": [
    "#### Type Token Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edb373e6-4416-4155-8e7d-69f48620dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_token_ratio(text):\n",
    "    \"\"\"\n",
    "    Calculate type-token ratio from the text using the first\n",
    "    num_words tokens\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A text for which we find the type-token ratio\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    type_token_ratio: int\n",
    "                    An integer which represents the type token ratio for a given text\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    num_words = 100\n",
    "    type_set = set(word.lower() for word in words[:num_words])\n",
    "    return len(type_set) / num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b1898434-d217-4f54-b7d1-c9160863d510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_type_token_ratio tests pass\n"
     ]
    }
   ],
   "source": [
    "s1 = \"\"\"Chelsea English School is offering a Summer School Program in Iwaki, Fukushima, a holiday learning experience combining enjoyment of the area's natural beauty and practical lifestyle immersion in the agricultural traditions of this part of Japan.\n",
    "We will be hosted by \"Namakiba\" farm, an agricultural concern run by an Iwaki City cooperative, and activities include handson experience of organic farming,barbecues, local nature sightseeing including swimming in the river and the sea,the local fish market, guesthouses with onsens (hot spas) . The program promises new and fresh experiences in both nature and culture, and time will also be made available for gift shopping. Non-Japanese speakers are also warmly invited, as simultaneous translation into English will be available throughout the e trip.\"\"\"\n",
    "\n",
    "s2 = \"\"\"I'd like to acquire this skill, however it doesn't really fit into my schedule right now. I'm still practicing a little bit every day.Well, I'll attend my exam within 8 weeks and a few days, but I've still many things to learn. That's what we call a challenge, so I find it quite interesting.\"\"\"\n",
    "assert get_type_token_ratio(s1) == 0.74\n",
    "assert get_type_token_ratio(s2) == 0.48\n",
    "\n",
    "print(\"get_type_token_ratio tests pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6358551f-23ac-4c81-a16f-9a03a7be3c7a",
   "metadata": {},
   "source": [
    "#### Asian Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5d9fd642-251a-4984-83af-7177c374fe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_asian_context_feature(text):\n",
    "    \"\"\"\n",
    "    Return a binary value based on whether asian journal's context based words are present in the text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A text for which we find the presence of the words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    value : boolean\n",
    "            0 represents that the no word in the list is present in the text and 1 represents vice versa.\n",
    "    \"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    with open(\"data/asian_words.txt\", \"r\") as file:\n",
    "        asian_journals_context_words = file.read().split(\"\\n\")\n",
    "\n",
    "    for word in word_tokenize(text):\n",
    "        if lemmatizer.lemmatize(word) in asian_journals_context_words:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aeb8ec91-3ea4-43de-8821-5af5f26c5c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_asian_context_feature tests pass\n"
     ]
    }
   ],
   "source": [
    "asian_journal_test = \"\"\"Frank moved to Guangzhou after long time consideration from another city. Finally he settled down in his new apartment and we had a welcome dinner together last night in a Indonesian restaurant which was absolutely new for me. \n",
    "I have not tried Indonesian food before but similar ones such as Singaporean and Malaysian dishes when I traveled there back to 2007. Southeast Asia food is full of spicy, curry tasting in common. I like it since the influence I got from my previous company, a Singapore based firm in Shanghai. I had good memory both about my pre-boss and good trip in Singapore and Malaysia. I was treated with lots of fun, adventure, foods and for sure can not be forgotten, humid and hot weather. \n",
    "Ok, back to the Indonesian restaurant. It is a tradtional one decorating with local stuff, french window, cane chair with the gentle local background music. I can smell the Indonesian in air. The dishes were really good and mostly important the price is fair too.\"\"\"\n",
    "\n",
    "asian_journal_test_2 = \"\"\"Today I had TV conference with Malaysian in English.I know we Japanese have strong accent ourselves, but I think for me their English is very difficult to understand as well. I would like to get used to their accent.\"\"\"\n",
    "\n",
    "european_journal_test = \"\"\"I am missing the friends, and I will miss my life in US. It hasn't been easy for me to stay here, mainly because of my English limitation, but now I wish I could stay longer here. Because I like California.\"\"\"\n",
    "\n",
    "european_journal_test_2 = \"\"\"A few days ago, I've discovered something pretty awesome: it is called penmanship.\n",
    "It could be decribed as the art of writing. After much practice, the results are really great.\"\"\"\n",
    "\n",
    "assert get_asian_context_feature(asian_journal_test) == 1\n",
    "assert get_asian_context_feature(european_journal_test) == 0\n",
    "assert get_asian_context_feature(asian_journal_test_2) == 1\n",
    "assert get_asian_context_feature(european_journal_test_2) == 0\n",
    "\n",
    "print(\"get_asian_context_feature tests pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d1f0ea-d658-42f9-8377-bff1d1760066",
   "metadata": {},
   "source": [
    "#### Word Importance (TF - IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c5f8c7ee-b089-4770-a389-02fd0a2c8ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_importance(text):\n",
    "    \"\"\"This is a helper function to generate TF IDF scores for words present in the input text\"\"\"\n",
    "    count = CountVectorizer(stop_words='english', analyzer='word')\n",
    "    word_count = count.fit_transform(text)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer(smooth_idf=True, use_idf=True)\n",
    "    tfidf_transformer.fit(word_count)\n",
    "    tf_idf_vector = tfidf_transformer.transform(word_count)\n",
    "\n",
    "    feature_names = count.get_feature_names_out()\n",
    "    first_document_vector = tf_idf_vector[0]\n",
    "\n",
    "    df_tfifd = pd.DataFrame(first_document_vector.T.todense(), index=feature_names, columns=[\"tfidf\"])\n",
    "    df_tfifd = df_tfifd.sort_values(by=[\"tfidf\"], ascending=False)\n",
    "\n",
    "    return df_tfifd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fce31dd-5a62-4084-9254-795bc88c7908",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_important_words():\n",
    "    \"\"\"This is a helper function to fetch words with top TF IDF scores for both asian and european journals\"\"\"\n",
    "\n",
    "    no_of_top_words = 30\n",
    "\n",
    "    asian_lang_text = []\n",
    "    non_asian_lang_text = []\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    asian_lang = [\"Korean\", \"Japanese\", \"Mandarin Chinese\"]\n",
    "\n",
    "    for extracted_data in extract_data_from_file():\n",
    "        if extracted_data['native_lang'] in asian_lang:\n",
    "            asian_lang_text.append(extracted_data['text'] + \", \")\n",
    "        else:\n",
    "            non_asian_lang_text.append(extracted_data['text'] + \", \")\n",
    "\n",
    "    asian_imp_words_df = get_word_importance(asian_lang_text)\n",
    "    non_asian_imp_words_df = get_word_importance(non_asian_lang_text)\n",
    "\n",
    "    asian_imp_words = asian_imp_words_df.index[:no_of_top_words]\n",
    "    asian_imp_words_lem = [lemmatizer.lemmatize(x) for x in asian_imp_words]\n",
    "\n",
    "    non_asian_imp_words = non_asian_imp_words_df.index[:no_of_top_words]\n",
    "    non_asian_imp_words_lem = [lemmatizer.lemmatize(x) for x in non_asian_imp_words]\n",
    "\n",
    "    return asian_imp_words_lem, non_asian_imp_words_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bddf229d-25fc-4c54-a80e-14db58bddeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "asian_imp_words_index, non_asian_imp_words_index = get_top_important_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbfebcb-5aeb-47d5-a3dc-f4d47c068cca",
   "metadata": {},
   "source": [
    "#### Asian Important Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "998a0c74-fbd4-4a22-b026-e70fd02c7463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imp_words_asian_feature(text):\n",
    "    \"\"\"\n",
    "    Return a binary value based on whether asian journal's important words (based on TF-IDF scores) are present in the text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A text for which we find the presence of the words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    value : boolean\n",
    "            0 represents that the no word in the list is present in the text and 1 represents vice versa.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for word in word_tokenize(text):\n",
    "        if lemmatizer.lemmatize(word) in asian_imp_words_index:\n",
    "            return 1\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f15a153-ade0-4025-b6fc-6f3581ecdab4",
   "metadata": {},
   "source": [
    "#### Non Asian Important Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d5b3b7b9-d581-48ea-81dd-2e9d9b47fb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imp_words_non_asian_feature(text):\n",
    "    \"\"\"\n",
    "    Return a binary value based on whether european journal's important words (based on TF-IDF scores) are present in the text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A text for which we find the presence of the words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    value : boolean\n",
    "            0 represents that the no word in the list is present in the text and 1 represents vice versa.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for word in word_tokenize(text):\n",
    "        if lemmatizer.lemmatize(word) in non_asian_imp_words_index:\n",
    "            return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27a85bdc-f018-483e-8359-50ca295555c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_imp_words_asian_feature tests pass\n",
      "get_imp_words_non_asian_feature tests pass\n"
     ]
    }
   ],
   "source": [
    "test_text_asian_feature = \"\"\"Hi, my name is Sebastián. This is the first time I write here at Lang-8, and to be honest it’s hard for me to decide what to talk about, but if you don’t make up your mind and practice you’ll never improve, so here I go…\n",
    "I’m studying English and Japanese. I use the computer and internet a lot for studying, which has given me access to several and great tools that every language learner should have access to, like blogs, forums, podcasts, sowftware and others..\n",
    "As for English, its pronunciation is hard, but reading isn’t that much hard. Besides, on internet there are amazing amounts of information in English about any subject. For example, I like stuff related to the use of computers and internet for both leisure and learning, and read sites like Lifehacker and Wikipedia and several others everyday. Actually, I’ve learned a lot of English by reading and writing at sites about learning Japanese.\n",
    "On the other side, reading Japanese is something that takes lots of time and effort. Even something as “simple” as reading the newspaper isn’t that “simple” at all. Fortunately, I study kanji using a book called “Remembering the Kanji” by James Heisig, and also spaced repetition softwares for kanji and Japanese in general, which has helped me a lot not just to actually learn, but also to feel much more motivated to keep on learning.\n",
    "As I said above, the use of the computer and internet has helped me a lot. I think this is the best moment to learn languages, as practically anything you need can be found on internet, and even for free. For example, Lang-8 is a great tool for every language learner, so I hope it keeps on having great success.\n",
    "What do you think?\"\"\"\n",
    "\n",
    "test_text_non_asian_feature = \"\"\"A few days ago, I've discovered something pretty awesome: it is called penmanship.\n",
    "It could be decribed as the art of writing. After much practice, the results are really great.\"\"\"\n",
    "\n",
    "\n",
    "assert get_imp_words_asian_feature(test_text_asian_feature) == 1\n",
    "assert get_imp_words_non_asian_feature(test_text_asian_feature) == 0\n",
    "assert get_imp_words_non_asian_feature(test_text_non_asian_feature) == 1\n",
    "assert get_imp_words_asian_feature(test_text_non_asian_feature) == 0\n",
    "\n",
    "print(\"get_imp_words_asian_feature tests pass\")\n",
    "print(\"get_imp_words_non_asian_feature tests pass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "895a60bf-5ab3-4a2b-86c6-1a07bbb84cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_list(txt_path):\n",
    "    \"\"\"\n",
    "    Extracts the list of documents stores in a text file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text_path : str\n",
    "        The string defining path of the text document\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A list of filenames extracted from the file\n",
    "    \"\"\"\n",
    "    doc_list = []\n",
    "\n",
    "    with open(txt_path, \"r\") as f:\n",
    "        for filename in f.readlines():\n",
    "            doc_list.append(filename.strip())\n",
    "\n",
    "    return doc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a86fda42-b453-4395-88cd-adce875c39b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_features(\n",
    "    txt_path, csv_path, zip_path=\"data/lang-8.zip\", verbose=False\n",
    "):\n",
    "    \"\"\"\n",
    "    Reads the zip file from path, extracts features from\n",
    "    preprocessed text and combines them together to save\n",
    "    them to a csv file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    csv_path : str\n",
    "            path at which the generated csv file is to be saved\n",
    "    zip_path : str\n",
    "            path to the zip file\n",
    "    verbose : boolean\n",
    "            specify whether to print the processed filename or not\n",
    "    \"\"\"\n",
    "\n",
    "    # Return if the file already exists\n",
    "    if os.path.isfile(csv_path):\n",
    "        return\n",
    "\n",
    "    # Lists of relevant features\n",
    "    asian_lang = [\"Korean\", \"Japanese\", \"Mandarin Chinese\"]\n",
    "    names = []\n",
    "    text_lens = []\n",
    "    lexical_densities = []\n",
    "    avg_sent_lens = []\n",
    "    avg_word_lens = []\n",
    "    oov_word_counts = []\n",
    "\n",
    "    reading_eases = []\n",
    "    punctuations_counts = []\n",
    "    type_token_ratios = []\n",
    "    asian_context_features = []\n",
    "    imp_words_asian_features = []\n",
    "    imp_words_non_asian_features = []\n",
    "\n",
    "    noun_counts, verb_counts, adj_counts = [], [], []\n",
    "    targets = []\n",
    "\n",
    "    # Lists of training, validation and test files\n",
    "    doc_list = get_document_list(txt_path)\n",
    "\n",
    "    for extracted_data in extract_data_from_file(zip_path):\n",
    "\n",
    "        if extracted_data[\"filename\"].removeprefix(\"lang-8/\") not in doc_list:\n",
    "            continue\n",
    "\n",
    "        if extracted_data[\"native_lang\"] == \"Russian\":\n",
    "            continue\n",
    "\n",
    "        if extracted_data['native_lang'] in asian_lang:\n",
    "            target = 1\n",
    "        else:\n",
    "            target = 0\n",
    "\n",
    "        targets.append(target)\n",
    "        names.append(extracted_data['filename'][7:-5])\n",
    "        text_lens.append(get_text_length(extracted_data['text']))\n",
    "        lexical_densities.append(get_lexical_density(extracted_data['text']))\n",
    "        avg_sent_lens.append(get_average_sentence_length(extracted_data['text']))\n",
    "        avg_word_lens.append(get_average_word_length(extracted_data['text']))\n",
    "        oov_word_counts.append(get_num_ovv_words(extracted_data['text']))\n",
    "\n",
    "        reading_eases.append(get_reading_ease(extracted_data['text']))\n",
    "        punctuations_counts.append(get_punctuations_count(extracted_data['text']))\n",
    "        type_token_ratios.append(get_type_token_ratio(extracted_data['text']))\n",
    "        asian_context_features.append(get_asian_context_feature(extracted_data['text']))\n",
    "        imp_words_asian_features.append(get_imp_words_asian_feature(extracted_data['text']))\n",
    "        imp_words_non_asian_features.append(get_imp_words_non_asian_feature(extracted_data['text']))\n",
    "\n",
    "        noun_count, verb_count, adj_count = get_pos_count(extracted_data['text'])\n",
    "        noun_counts.append(noun_count)\n",
    "        verb_counts.append(verb_count)\n",
    "        adj_counts.append(adj_count)\n",
    "\n",
    "        if verbose:\n",
    "            print(len(targets), extracted_data[\"filename\"])\n",
    "\n",
    "    feature_df = pd.DataFrame(\n",
    "        np.array([\n",
    "            names,\n",
    "            text_lens,\n",
    "            lexical_densities,\n",
    "            avg_sent_lens,\n",
    "            avg_word_lens,\n",
    "            oov_word_counts,\n",
    "\n",
    "            reading_eases,\n",
    "            punctuations_counts,\n",
    "            type_token_ratios,\n",
    "            asian_context_features,\n",
    "            imp_words_asian_features,\n",
    "            imp_words_non_asian_features,\n",
    "\n",
    "            noun_counts,\n",
    "            verb_counts,\n",
    "            adj_counts,\n",
    "            targets\n",
    "        ]).T,\n",
    "        columns=[\n",
    "            \"filename\",\n",
    "            \"text_length\",\n",
    "            \"lexical_density\",\n",
    "            \"average_sentence_length\",\n",
    "            \"average_word_length\",\n",
    "            \"oov_word_counts\",\n",
    "\n",
    "            \"reading_ease\",\n",
    "            \"punctuation_count\",\n",
    "            \"type_token_ratio\",\n",
    "            \"asian_context_feature\",\n",
    "            \"asian_imp_word\",\n",
    "            \"non_asian_imp_word\",\n",
    "\n",
    "            \"noun_counts\",\n",
    "            \"verb_counts\",\n",
    "            \"adjective_counts\",\n",
    "            \"target_region\"\n",
    "        ])\n",
    "\n",
    "    feature_df.to_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "141eec2c-103f-4d24-b603-0649a1a928b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dev_test_csvs(paths={\n",
    "                                    \"data/train.txt\": \"data/train.csv\",\n",
    "                                    \"data/dev.txt\": \"data/dev.csv\",\n",
    "                                    \"data/test.txt\": \"data/test.csv\"\n",
    "                                },\n",
    "                               zip_path=\"data/lang-8.zip\"):\n",
    "    \"\"\"\n",
    "    Takes in paths of text documents containing filenames from which\n",
    "    information is to be extracted, extracts informtion from them and\n",
    "    store them as csvs for train, validation and test\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    paths : dict\n",
    "        a dictionary with keys as paths for text documents to read filenames\n",
    "        and values as paths for the csv documents to save extracted features\n",
    "    zip_path : str\n",
    "            path to the zip file\n",
    "    \"\"\"\n",
    "\n",
    "    for txt_path, csv_path in paths.items():\n",
    "        extract_all_features(txt_path, csv_path, zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "86581849-e608-4861-aba9-f865d31022b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csvs(train, val, test):\n",
    "    \"\"\"\n",
    "    Reads train, validation and test sets from disk\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train : str\n",
    "        The path of the training csv file\n",
    "    train : str\n",
    "        The path of the training csv file\n",
    "    train : str\n",
    "        The path of the training csv file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of train, validation and test dataframes\n",
    "    \"\"\"\n",
    "    train_csv = None\n",
    "    val_csv = None\n",
    "    test_csv = None\n",
    "\n",
    "    try:\n",
    "        train_csv = pd.read_csv(train)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        val_csv = pd.read_csv(val)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        test_csv = pd.read_csv(test)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    return train_csv, val_csv, test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2c0e3a80-5cf3-4c23-86c0-07eb81efc6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data\n",
    "\n",
    "train_csv_path = r\"data/train.csv\"\n",
    "val_csv_path = r\"data/dev.csv\"\n",
    "test_csv_path = r\"data/test.csv\"\n",
    "\n",
    "train_txt_path = r\"data/train.txt\"\n",
    "val_txt_path = r\"data/dev.txt\"\n",
    "test_txt_path = r\"data/test.txt\"\n",
    "\n",
    "paths = {\n",
    "    train_txt_path: train_csv_path,\n",
    "    val_txt_path: val_csv_path,\n",
    "    test_txt_path: test_csv_path\n",
    "}\n",
    "\n",
    "zip_path = r\"data/lang-8.zip\"\n",
    "\n",
    "if not (os.path.isfile(\n",
    "    train_csv_path\n",
    ") and os.path.isfile(\n",
    "    val_csv_path\n",
    ") and os.path.isfile(\n",
    "    test_csv_path\n",
    ")):\n",
    "    create_train_dev_test_csvs(paths)\n",
    "\n",
    "train_data, val_data, test_data = read_csvs(train_csv_path, val_csv_path, test_csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50990767-a146-4953-b2df-598065ab0545",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "48764d86-6e7b-40b8-82da-4200a87c3bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_length</th>\n",
       "      <th>lexical_density</th>\n",
       "      <th>average_sentence_length</th>\n",
       "      <th>average_word_length</th>\n",
       "      <th>oov_word_counts</th>\n",
       "      <th>reading_ease</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>type_token_ratio</th>\n",
       "      <th>asian_context_feature</th>\n",
       "      <th>asian_imp_word</th>\n",
       "      <th>non_asian_imp_word</th>\n",
       "      <th>noun_counts</th>\n",
       "      <th>verb_counts</th>\n",
       "      <th>adjective_counts</th>\n",
       "      <th>target_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>281</td>\n",
       "      <td>0.480427</td>\n",
       "      <td>70.250000</td>\n",
       "      <td>4.259786</td>\n",
       "      <td>21</td>\n",
       "      <td>18.717015</td>\n",
       "      <td>18</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>41</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29</td>\n",
       "      <td>0.655172</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>4.448276</td>\n",
       "      <td>1</td>\n",
       "      <td>46.124138</td>\n",
       "      <td>1</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>307</td>\n",
       "      <td>0.592834</td>\n",
       "      <td>38.375000</td>\n",
       "      <td>4.566775</td>\n",
       "      <td>17</td>\n",
       "      <td>37.815320</td>\n",
       "      <td>20</td>\n",
       "      <td>0.77</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>95</td>\n",
       "      <td>52</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67</td>\n",
       "      <td>0.611940</td>\n",
       "      <td>22.333333</td>\n",
       "      <td>4.104478</td>\n",
       "      <td>5</td>\n",
       "      <td>57.898010</td>\n",
       "      <td>13</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "      <td>15</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25</td>\n",
       "      <td>0.520000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>3.520000</td>\n",
       "      <td>0</td>\n",
       "      <td>79.940000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>107</td>\n",
       "      <td>0.532710</td>\n",
       "      <td>53.500000</td>\n",
       "      <td>4.130841</td>\n",
       "      <td>6</td>\n",
       "      <td>29.981098</td>\n",
       "      <td>5</td>\n",
       "      <td>0.74</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>26</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>64</td>\n",
       "      <td>0.671875</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>4.296875</td>\n",
       "      <td>0</td>\n",
       "      <td>48.776875</td>\n",
       "      <td>13</td>\n",
       "      <td>0.49</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>13</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>63</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>3.952381</td>\n",
       "      <td>3</td>\n",
       "      <td>70.034286</td>\n",
       "      <td>6</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>37</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>4.567568</td>\n",
       "      <td>2</td>\n",
       "      <td>29.804324</td>\n",
       "      <td>2</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>146</td>\n",
       "      <td>0.547945</td>\n",
       "      <td>7.300000</td>\n",
       "      <td>4.116438</td>\n",
       "      <td>13</td>\n",
       "      <td>69.628240</td>\n",
       "      <td>68</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>22</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>743 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     text_length  lexical_density  average_sentence_length  \\\n",
       "0            281         0.480427                70.250000   \n",
       "1             29         0.655172                29.000000   \n",
       "2            307         0.592834                38.375000   \n",
       "3             67         0.611940                22.333333   \n",
       "4             25         0.520000                25.000000   \n",
       "..           ...              ...                      ...   \n",
       "738          107         0.532710                53.500000   \n",
       "739           64         0.671875                32.000000   \n",
       "740           63         0.523810                21.000000   \n",
       "741           37         0.702703                37.000000   \n",
       "742          146         0.547945                 7.300000   \n",
       "\n",
       "     average_word_length  oov_word_counts  reading_ease  punctuation_count  \\\n",
       "0               4.259786               21     18.717015                 18   \n",
       "1               4.448276                1     46.124138                  1   \n",
       "2               4.566775               17     37.815320                 20   \n",
       "3               4.104478                5     57.898010                 13   \n",
       "4               3.520000                0     79.940000                  2   \n",
       "..                   ...              ...           ...                ...   \n",
       "738             4.130841                6     29.981098                  5   \n",
       "739             4.296875                0     48.776875                 13   \n",
       "740             3.952381                3     70.034286                  6   \n",
       "741             4.567568                2     29.804324                  2   \n",
       "742             4.116438               13     69.628240                 68   \n",
       "\n",
       "     type_token_ratio  asian_context_feature  asian_imp_word  \\\n",
       "0                0.61                      0               0   \n",
       "1                0.22                      0               0   \n",
       "2                0.77                      0               1   \n",
       "3                0.50                      0               1   \n",
       "4                0.18                      0               0   \n",
       "..                ...                    ...             ...   \n",
       "738              0.74                      1               0   \n",
       "739              0.49                      1               0   \n",
       "740              0.49                      0               0   \n",
       "741              0.30                      0               0   \n",
       "742              0.65                      0               0   \n",
       "\n",
       "     non_asian_imp_word  noun_counts  verb_counts  adjective_counts  \\\n",
       "0                     1           79           41                11   \n",
       "1                     0            5            7                 5   \n",
       "2                     0           95           52                14   \n",
       "3                     0           18           15                 6   \n",
       "4                     1            4            7                 1   \n",
       "..                  ...          ...          ...               ...   \n",
       "738                   0           19           26                 4   \n",
       "739                   0           20           13                 7   \n",
       "740                   1           13           14                 1   \n",
       "741                   1           10           12                 3   \n",
       "742                   0           35           22                16   \n",
       "\n",
       "     target_region  \n",
       "0                0  \n",
       "1                0  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  \n",
       "..             ...  \n",
       "738              1  \n",
       "739              1  \n",
       "740              1  \n",
       "741              1  \n",
       "742              0  \n",
       "\n",
       "[743 rows x 15 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = train_data.drop(columns=[\"Unnamed: 0\", \"filename\"])\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a7bfaee-8189-4b2c-8153-d417d83ea165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_length</th>\n",
       "      <th>lexical_density</th>\n",
       "      <th>average_sentence_length</th>\n",
       "      <th>average_word_length</th>\n",
       "      <th>oov_word_counts</th>\n",
       "      <th>reading_ease</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>type_token_ratio</th>\n",
       "      <th>asian_context_feature</th>\n",
       "      <th>asian_imp_word</th>\n",
       "      <th>non_asian_imp_word</th>\n",
       "      <th>noun_counts</th>\n",
       "      <th>verb_counts</th>\n",
       "      <th>adjective_counts</th>\n",
       "      <th>target_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>699</td>\n",
       "      <td>0.595136</td>\n",
       "      <td>14.265306</td>\n",
       "      <td>4.214592</td>\n",
       "      <td>40</td>\n",
       "      <td>69.147131</td>\n",
       "      <td>87</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>158</td>\n",
       "      <td>157</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>618</td>\n",
       "      <td>0.600324</td>\n",
       "      <td>22.888889</td>\n",
       "      <td>4.872168</td>\n",
       "      <td>34</td>\n",
       "      <td>44.108603</td>\n",
       "      <td>61</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>173</td>\n",
       "      <td>88</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83</td>\n",
       "      <td>0.590361</td>\n",
       "      <td>20.750000</td>\n",
       "      <td>4.421687</td>\n",
       "      <td>5</td>\n",
       "      <td>53.267726</td>\n",
       "      <td>13</td>\n",
       "      <td>0.67</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>4.406250</td>\n",
       "      <td>1</td>\n",
       "      <td>50.098750</td>\n",
       "      <td>3</td>\n",
       "      <td>0.27</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70</td>\n",
       "      <td>0.585714</td>\n",
       "      <td>23.333333</td>\n",
       "      <td>4.828571</td>\n",
       "      <td>4</td>\n",
       "      <td>50.208810</td>\n",
       "      <td>14</td>\n",
       "      <td>0.51</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>13</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>73</td>\n",
       "      <td>0.602740</td>\n",
       "      <td>12.166667</td>\n",
       "      <td>3.849315</td>\n",
       "      <td>1</td>\n",
       "      <td>80.913231</td>\n",
       "      <td>7</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>88</td>\n",
       "      <td>0.579545</td>\n",
       "      <td>12.571429</td>\n",
       "      <td>3.829545</td>\n",
       "      <td>2</td>\n",
       "      <td>82.556818</td>\n",
       "      <td>13</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>37</td>\n",
       "      <td>0.648649</td>\n",
       "      <td>18.500000</td>\n",
       "      <td>4.810811</td>\n",
       "      <td>0</td>\n",
       "      <td>41.722365</td>\n",
       "      <td>9</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>322</td>\n",
       "      <td>0.562112</td>\n",
       "      <td>21.466667</td>\n",
       "      <td>4.760870</td>\n",
       "      <td>14</td>\n",
       "      <td>57.095402</td>\n",
       "      <td>59</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79</td>\n",
       "      <td>53</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>198</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>4.287879</td>\n",
       "      <td>6</td>\n",
       "      <td>44.303636</td>\n",
       "      <td>20</td>\n",
       "      <td>0.70</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>246 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     text_length  lexical_density  average_sentence_length  \\\n",
       "0            699         0.595136                14.265306   \n",
       "1            618         0.600324                22.888889   \n",
       "2             83         0.590361                20.750000   \n",
       "3             32         0.625000                32.000000   \n",
       "4             70         0.585714                23.333333   \n",
       "..           ...              ...                      ...   \n",
       "241           73         0.602740                12.166667   \n",
       "242           88         0.579545                12.571429   \n",
       "243           37         0.648649                18.500000   \n",
       "244          322         0.562112                21.466667   \n",
       "245          198         0.555556                33.000000   \n",
       "\n",
       "     average_word_length  oov_word_counts  reading_ease  punctuation_count  \\\n",
       "0               4.214592               40     69.147131                 87   \n",
       "1               4.872168               34     44.108603                 61   \n",
       "2               4.421687                5     53.267726                 13   \n",
       "3               4.406250                1     50.098750                  3   \n",
       "4               4.828571                4     50.208810                 14   \n",
       "..                   ...              ...           ...                ...   \n",
       "241             3.849315                1     80.913231                  7   \n",
       "242             3.829545                2     82.556818                 13   \n",
       "243             4.810811                0     41.722365                  9   \n",
       "244             4.760870               14     57.095402                 59   \n",
       "245             4.287879                6     44.303636                 20   \n",
       "\n",
       "     type_token_ratio  asian_context_feature  asian_imp_word  \\\n",
       "0                0.70                      0               1   \n",
       "1                0.70                      0               1   \n",
       "2                0.67                      1               1   \n",
       "3                0.27                      1               0   \n",
       "4                0.51                      1               1   \n",
       "..                ...                    ...             ...   \n",
       "241              0.53                      0               0   \n",
       "242              0.63                      0               1   \n",
       "243              0.33                      0               0   \n",
       "244              0.81                      0               1   \n",
       "245              0.70                      0               1   \n",
       "\n",
       "     non_asian_imp_word  noun_counts  verb_counts  adjective_counts  \\\n",
       "0                     1          158          157                51   \n",
       "1                     1          173           88                53   \n",
       "2                     0           20           17                 9   \n",
       "3                     0            7            9                 2   \n",
       "4                     0           25           13                 5   \n",
       "..                  ...          ...          ...               ...   \n",
       "241                   1           12           16                 8   \n",
       "242                   0           19           19                10   \n",
       "243                   0           13            8                 3   \n",
       "244                   1           79           53                39   \n",
       "245                   1           40           40                20   \n",
       "\n",
       "     target_region  \n",
       "0                1  \n",
       "1                0  \n",
       "2                1  \n",
       "3                0  \n",
       "4                1  \n",
       "..             ...  \n",
       "241              1  \n",
       "242              1  \n",
       "243              1  \n",
       "244              1  \n",
       "245              1  \n",
       "\n",
       "[246 rows x 15 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data = val_data.drop(columns=[\"Unnamed: 0\", \"filename\"])\n",
    "val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1835d3-8377-4551-9645-863d245e4e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_data.drop(columns=[\"Unnamed: 0\", \"filename\"])\n",
    "test_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:573]",
   "language": "python",
   "name": "conda-env-573-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
