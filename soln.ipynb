{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52675a9e-b72c-4215-967a-a5406225aea4",
   "metadata": {},
   "source": [
    "## L2 Language Learner Classification "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c809d722-dcd3-41fd-a4cb-4db4b88eccd5",
   "metadata": {},
   "source": [
    "Building a classifier to distinguish English text written by Lang-8 users whose native language (L1) is another European language (French and Spanish) from those written by L1 speakers of East Asian languages (Japanese, Korean, and Mandarin Chinese)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df05892-619d-408e-abb1-5cd6094d7a9a",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d76234b1-4cb7-416b-8a67-30b83d1794de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/snehajhaveri/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package cmudict to\n",
      "[nltk_data]     /Users/snehajhaveri/nltk_data...\n",
      "[nltk_data]   Package cmudict is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from zipfile import ZipFile\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('words')\n",
    "nltk.download(\"cmudict\")\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import cmudict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a871b9-6068-4704-8300-d70f40257cd5",
   "metadata": {},
   "source": [
    "### Text Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86b12526-70de-4821-a4d7-c90695a73586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file_from_zip(path=\"data/lang-8.zip\"):\n",
    "    \"\"\"\n",
    "    A generator function which reads html documents\n",
    "    as raw text from the zip file\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : string\n",
    "        path to the zip file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary of filename and raw text extracted\n",
    "    from the file\n",
    "    \"\"\"\n",
    "    archive = ZipFile(path, \"r\")\n",
    "\n",
    "    for file in archive.namelist()[1:]:\n",
    "        yield {\n",
    "            \"filename\": file,#.removeprefix(\"lang-8/\"),\n",
    "            \"data\": archive.read(file)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd7d5e33-770f-47fe-87a5-b6eb8af54c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_from_file(path=\"data/lang-8.zip\"):\n",
    "    \"\"\"\n",
    "    A generator function which reads html files from zip\n",
    "    and extracts text and native language from the raw\n",
    "    text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : string\n",
    "        path to the zip file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A dictionary of extracted content and native language\n",
    "    of the author\n",
    "    \"\"\"\n",
    "    for data_dict in read_file_from_zip(path):\n",
    "        soup = BeautifulSoup(data_dict[\"data\"])\n",
    "\n",
    "#         author = soup.find_all(\"p\", attrs={\"class\": \"spaced\"})[1].get_text().strip()\n",
    "        native_lang = soup.find(\"li\", attrs={\"data-title\": \"Native language\"}).get_text().strip()\n",
    "        filename = data_dict[\"filename\"]\n",
    "        text = soup.find(\"div\", attrs={\"id\": \"body_show_ori\"}).get_text().strip()\n",
    "\n",
    "        preprocessed_data = {\n",
    "            \"text\": text,\n",
    "#             \"author\": author,\n",
    "            \"native_lang\": native_lang,\n",
    "            \"filename\": filename\n",
    "        }\n",
    "\n",
    "        yield preprocessed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d397f65-6fd0-4ddc-9900-f7367a3cbac2",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed28333-976b-4add-803d-0fdaeed686f7",
   "metadata": {},
   "source": [
    "#### Text Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b14e13d-15b8-4f44-8534-f8f38b613145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_length(text):\n",
    "    \"\"\"\n",
    "    Returns the number of words in a text without punctuations. \n",
    "    Counts clitics as separate words.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A text from which we find the number of words\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    An int which represents the number of words in the text\n",
    "    \"\"\"\n",
    "    non_punc = []\n",
    "    for word in word_tokenize(text):\n",
    "        if word not in string.punctuation:\n",
    "            non_punc.append(word)\n",
    "    return len(non_punc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e865286-7ada-4fd7-8a12-5ef9e5b8ecdb",
   "metadata": {},
   "source": [
    "#### Lexical Density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3a22f923-249b-4274-8656-2dd0dd606b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lexical_density(text):\n",
    "    \"\"\"\n",
    "    Returns the lexical density of a text. That is the ratio of open class words.\n",
    "    in the text\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A text from which we find the lexical density\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A float which represents the lexical density\n",
    "    \"\"\"\n",
    "    open_class_prefix = {\"N\", \"V\", \"J\", \"R\"}\n",
    "    open_class_total = 0\n",
    "    word_count = 0\n",
    "    if len(text) == 0:\n",
    "        return float(0)\n",
    "    for word, pos in pos_tag(word_tokenize(text)):\n",
    "        if word not in string.punctuation:\n",
    "            word_count += 1\n",
    "            if pos[0] in open_class_prefix:\n",
    "                open_class_total += 1\n",
    "    return open_class_total/word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d5e4bd-21bd-4ae2-947a-6859dd328761",
   "metadata": {},
   "source": [
    "#### Average Sentence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bc3d0d3-12be-4d5e-a576-0030aa1a23d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_sentence_length(text):\n",
    "    \"\"\"\n",
    "    Returns the average sentence length of a text. Does not count punctuations and counts clitics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A text from which we find the average sentence length\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A float which represents the average sentence length\n",
    "    \"\"\"\n",
    "    if len(text) == 0:\n",
    "        return float(0)\n",
    "    sent_lengths = 0\n",
    "    for sentence in sent_tokenize(text):\n",
    "        word_count = 0\n",
    "        for word in word_tokenize(sentence):\n",
    "            if word not in string.punctuation:\n",
    "                word_count += 1\n",
    "        sent_lengths += word_count\n",
    "    return sent_lengths/len(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d884d82-1b29-4efc-a2c9-c1ee617700ac",
   "metadata": {},
   "source": [
    "#### Average Word Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c51658f4-3e59-4d4b-8b54-8b4b750d04fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_word_length(text):\n",
    "    \"\"\"\n",
    "    Returns the average sentence length of a text. Does not count punctuations \n",
    "    and counts clitics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A text from which we find the average sentence length\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A float which represents the average sentence length\n",
    "    \"\"\"\n",
    "    if len(text) == 0:\n",
    "        return float(0)\n",
    "    word_count = 0\n",
    "    lengths_sum = 0\n",
    "    for word in word_tokenize(text):\n",
    "        if word not in string.punctuation:\n",
    "            lengths_sum += len(word)\n",
    "            word_count += 1\n",
    "    return lengths_sum/word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c585d982-cd1f-45a8-905d-80a03cadeece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_text_length tests pass\n",
      "get_lexical_density tests pass\n",
      "get_average_sentence_length tests pass\n",
      "get_average_word_length tests pass\n"
     ]
    }
   ],
   "source": [
    "s0 = \"\"\n",
    "s1 = \"\"\"I went to the park today. \n",
    "I love going there because I always have so much fun. \n",
    "I invited some friends but they didn't come. \n",
    "That's fine because I met a new person there. \n",
    "He had a dog.\n",
    "\"\"\" #40, \n",
    "s2 = \"I have so much work to do today. I am stressed\" #11\n",
    "\n",
    "# get_text_length\n",
    "assert type(get_text_length(s0)) == int, \"Must be an interger\"\n",
    "assert get_text_length(s0) == 0, \"Empty string must return 0\"\n",
    "assert get_text_length(s1) == 40, \"s1 has 40 words\"\n",
    "assert get_text_length(s2) == 11, \"s2 has 11 words\"\n",
    "print(\"get_text_length tests pass\")\n",
    "\n",
    "assert type(get_lexical_density(s0))== float, \"Must be a float\"\n",
    "assert get_lexical_density(s0) == 0, \"Empty string must return 0\"\n",
    "assert get_lexical_density(s1) == 24/40, \"24 open class words out of 40\"\n",
    "assert get_lexical_density(s2) == 8/11, \"8 open class words out of 40\"\n",
    "print(\"get_lexical_density tests pass\")\n",
    "\n",
    "assert type(get_average_sentence_length(s0)) == float, \"Must be a float\"\n",
    "assert get_average_sentence_length(s0) == 0, \"Empty string must return 0\"\n",
    "assert get_average_sentence_length(s1) == 40/5, \"40 words over the span of 5 sentences\"\n",
    "assert get_average_sentence_length(s2) == 11/2, \"11 words over the span 2 sentences\"\n",
    "print(\"get_average_sentence_length tests pass\")\n",
    "\n",
    "assert type(get_average_word_length(s0)) == float, \"Must be a float\"\n",
    "assert get_average_word_length(s0) == 0, \"Empty string must return 0\"\n",
    "assert get_average_word_length(s1) == 142/40, \"142 total characters spread across 40 words\"\n",
    "assert get_average_word_length(s2) == 35/11, \"35 character spread across 11 words\"\n",
    "print(\"get_average_word_length tests pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c960467-2b5a-4182-b76e-2495f7366686",
   "metadata": {},
   "source": [
    "#### Part of Speech (POS) Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cee2ba5-73ef-484a-92fe-d0798afbdc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_count(text):\n",
    "    \"\"\"\n",
    "    Counts the number of nouns, verbs and adjectives in a text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A text for which we find the number of nouns, verbs\n",
    "        and adjectives\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    A tuple of (noun_count: int, verb_count: int, adj_count: int)\n",
    "    which represents the number of nouns, verbs adjectives in the text\n",
    "    respectively\n",
    "    \"\"\"\n",
    "    noun_count = 0\n",
    "    verb_count = 0\n",
    "    adj_count = 0\n",
    "\n",
    "    if len(text) == 0:\n",
    "        return 0, 0, 0\n",
    "\n",
    "    for word, pos in pos_tag(word_tokenize(text)):\n",
    "        if(pos[0] == 'N'):\n",
    "            noun_count += 1\n",
    "        if(pos[0] == 'V'):\n",
    "            verb_count += 1\n",
    "        if(pos == 'JJ'):\n",
    "            adj_count += 1\n",
    "    return noun_count, verb_count, adj_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7d2ae12-f039-4622-af96-347d637cf359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_pos_count tests pass\n"
     ]
    }
   ],
   "source": [
    "s1 = \"\"\"I went to the park today. \n",
    "I love going there because I always have so much fun. \n",
    "I invited some friends but they didn't come. \n",
    "That's fine because I met a new person there. \n",
    "He had a dog.\"\"\"\n",
    "\n",
    "s2 = \"\"\"Chelsea English School is offering a Summer School Program in Iwaki, Fukushima, a holiday learning experience combining enjoyment of the area's natural beauty and practical lifestyle immersion in the agricultural traditions of this part of Japan.\n",
    "We will be hosted by \"Namakiba\" farm, an agricultural concern run by an Iwaki City cooperative, and activities include handson experience of organic farming,barbecues, local nature sightseeing including swimming in the river and the sea,the local fish market, guesthouses with onsens (hot spas) . The program promises new and fresh experiences in both nature and culture, and time will also be made available for gift shopping. Non-Japanese speakers are also warmly invited, as simultaneous translation into English will be available throughout the e trip.\"\"\"\n",
    "\n",
    "assert get_pos_count(s1) == (6, 10, 3)\n",
    "assert get_pos_count(s2) == (47, 17, 16)\n",
    "\n",
    "print(\"get_pos_count tests pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcce884e-f656-438e-bdf8-c9b8fd9fd66d",
   "metadata": {},
   "source": [
    "#### Out of Vocabulary Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "def826d3-06f3-49ef-9122-95043d35db39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_num_ovv_words(text):\n",
    "    \"\"\"\n",
    "    Gets the number of out-of-vocabulary words in a text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A text for which we find the number of out-of-vocabulary\n",
    "        words is to be found\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The number of oov words in the text\n",
    "    \"\"\"\n",
    "    text_vocab = set(w.lower() for w in text.split() if w.isalpha())\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    ovv_words = text_vocab - english_vocab\n",
    "\n",
    "    return len(ovv_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce181ec3-f831-434b-b855-1e1fca378a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_num_ovv_words tests pass\n"
     ]
    }
   ],
   "source": [
    "s0 = \"\"\n",
    "s1 = \"\"\" I haddd to leaasve earliae since yesterday was so tired.\n",
    "And then I met you.\n",
    "\"\"\" \n",
    "s2 = \"I have so much work to do today. I am stressseed\"\n",
    "assert type(get_num_ovv_words(s0)) == int, \"Must be an interger\"\n",
    "assert get_num_ovv_words(s0) == 0, \"Empty string must return 0\"\n",
    "assert get_num_ovv_words(s1) == 3, \"s1 has 3 words out of vaocab\"\n",
    "assert get_num_ovv_words(s2) == 1, \"s2 has 1 words out of vocab\"\n",
    "print(\"get_num_ovv_words tests pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc565f4-279e-4c7a-8a32-b68dd58afa38",
   "metadata": {},
   "source": [
    "#### Reading Ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a6854544-8e1c-4019-899b-fb23421f32b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Code adapted from lab\n",
    "\n",
    "vowels = {\"a\",\"e\",\"i\",\"o\",\"u\",\"y\"}\n",
    "p_dict = cmudict.dict()\n",
    "\n",
    "def get_reading_ease(text):\n",
    "    \"\"\"Returns the reading ease for a text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "       A text for which we find the reading ease.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    reading_ease : float\n",
    "        The reading ease for the text\n",
    "    \"\"\"\n",
    "    syllable_count = 0\n",
    "    word_count = 0\n",
    "\n",
    "    for word in word_tokenize(text):\n",
    "        if word not in string.punctuation:\n",
    "            word_count += 1\n",
    "            if word in p_dict:\n",
    "                for pron in p_dict[word][0]:\n",
    "                    if pron[-1] in ['0','1','2']:\n",
    "                        syllable_count +=1\n",
    "            else:\n",
    "                for j in range(0,len(word)):\n",
    "                    if word[j].lower() in vowels:\n",
    "                         syllable_count= syllable_count+1\n",
    "\n",
    "    reading_ease = (206.835 - (1.015*(word_count/len(sent_tokenize(text))))- (84.6*(syllable_count/word_count)))\n",
    "    return reading_ease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee8e7141-0586-4c7f-9ffb-b56310d818bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_reading_ease tests pass\n"
     ]
    }
   ],
   "source": [
    "assert 100 < get_reading_ease(\"I am done, man\") < 140\n",
    "assert -60 < get_reading_ease(\"Felicitations for achieving a thoroughly excellent resolution to an altogether indombidable conundrum of humongous proportions.\") <-20\n",
    "print(\"get_reading_ease tests pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c2989-7726-49d7-b66a-6fd26883aa79",
   "metadata": {},
   "source": [
    "#### Punctuation Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "976bf111-d6fa-461b-bcae-e268bad939d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_punctuations_count(text):\n",
    "    \"\"\"\n",
    "    Returns the number of punctuations in a text.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A text for which we find the number of punctuations present\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    punct_count: int\n",
    "                 An integer which represents the number of punctuations in the text\n",
    "    \"\"\"\n",
    "    punct_count = 0\n",
    "    if len(text) == 0:\n",
    "        return 0\n",
    "    for word in word_tokenize(text):\n",
    "        if word in string.punctuation:\n",
    "            punct_count += 1\n",
    "    return punct_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86852d5c-9062-447f-aa3f-19e0d5af07ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_punctuations_count tests pass\n"
     ]
    }
   ],
   "source": [
    "s1 = \"\"\"I went to the park today. \n",
    "I love going there because I always have so much fun. \n",
    "I invited some friends but they didn't come. \n",
    "That's fine because I met a new person there. \n",
    "He had a dog.\"\"\"\n",
    "\n",
    "s2 = \"\"\"Chelsea English School is offering a Summer School Program in Iwaki, Fukushima, a holiday learning experience combining enjoyment of the area's natural beauty and practical lifestyle immersion in the agricultural traditions of this part of Japan.\n",
    "We will be hosted by \"Namakiba\" farm, an agricultural concern run by an Iwaki City cooperative, and activities include handson experience of organic farming,barbecues, local nature sightseeing including swimming in the river and the sea,the local fish market, guesthouses with onsens (hot spas) . The program promises new and fresh experiences in both nature and culture, and time will also be made available for gift shopping. Non-Japanese speakers are also warmly invited, as simultaneous translation into English will be available throughout the e trip.\"\"\"\n",
    "\n",
    "\n",
    "assert get_punctuations_count(s1) == 5\n",
    "assert get_punctuations_count(s2) == 16\n",
    "\n",
    "print(\"get_punctuations_count tests pass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226da160-97ce-4a9e-aca3-b37fefd65ecd",
   "metadata": {},
   "source": [
    "#### Type Token Ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "edb373e6-4416-4155-8e7d-69f48620dcfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_type_token_ratio(text):\n",
    "    \"\"\"\n",
    "    Calculate type-token ratio from the text using the first\n",
    "    num_words tokens\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        A text for which we find the type-token ratio\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    type_token_ratio: int\n",
    "                    An integer which represents the type token ratio for a given text\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    num_words = 100\n",
    "    type_set = set(word.lower() for word in words[:num_words])\n",
    "    return len(type_set) / num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1898434-d217-4f54-b7d1-c9160863d510",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:573]",
   "language": "python",
   "name": "conda-env-573-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
